{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb9ccbfe-8274-47ce-ad18-567438beb995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1...\n",
      "Fetching page 2...\n",
      "Fetching page 3...\n",
      "Fetching page 4...\n",
      "Fetching page 5...\n",
      "Fetching page 6...\n",
      "Fetching page 7...\n",
      "Fetching page 8...\n",
      "Fetching page 9...\n",
      "Fetching page 10...\n",
      "Fetching page 11...\n",
      "Fetching page 12...\n",
      "Fetching page 13...\n",
      "Fetching page 14...\n",
      "Fetching page 15...\n",
      "Fetching page 16...\n",
      "Fetching page 17...\n",
      "Fetching page 18...\n",
      "Fetching page 19...\n",
      "Fetching page 20...\n",
      "Data appended to 'amazon_products_database.csv'.\n",
      "Database loaded successfully.\n",
      "Database cleaned and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Base URL for Amazon search results (modify the search query as needed)\n",
    "BASE_URL = \"https://www.amazon.in/s?bbn=21541572031&rh=n%3A976419031&dc&qid=1733993930&rnid=3576079031&ref=sr_nr_n_0\"\n",
    "\n",
    "# Headers to mimic a browser (avoid being blocked)\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "}\n",
    "\n",
    "# Function to fetch product details\n",
    "def fetch_product_data(page):\n",
    "    url = f\"{BASE_URL}&page={page}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch page {page}: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    products = soup.find_all(\"div\", {\"data-component-type\": \"s-search-result\"})\n",
    "\n",
    "    data = []\n",
    "    for product in products:\n",
    "        try:\n",
    "            # Extract product title\n",
    "            title_tag = product.find(\"h2\")\n",
    "            title = title_tag.text.strip() if title_tag else \"Title Not Available\"\n",
    "\n",
    "            # Extract price\n",
    "            price_tag = product.find(\"span\", \"a-price-whole\")\n",
    "            price = int(re.sub(r'[^\\d]', '', price_tag.text)) if price_tag else None\n",
    "\n",
    "            # Extract original price\n",
    "            original_price_tag = product.find(\"span\", \"a-price a-text-price\")\n",
    "            original_price = (\n",
    "                int(re.sub(r'[^\\d]', '', original_price_tag.find(\"span\", \"a-offscreen\").text))\n",
    "                if original_price_tag and original_price_tag.find(\"span\", \"a-offscreen\")\n",
    "                else None\n",
    "            )\n",
    "\n",
    "            # Calculate discount percentage\n",
    "            if price and original_price:\n",
    "                discount = round(((original_price - price) / original_price) * 100, 2)\n",
    "            else:\n",
    "                discount = None\n",
    "\n",
    "            # Extract rating\n",
    "            rating_tag = product.find(\"span\", \"a-icon-alt\")\n",
    "            rating = rating_tag.text.split()[0] if rating_tag else None\n",
    "\n",
    "            # Extract number of reviews\n",
    "            reviews_tag = product.find(\"span\", {\"class\": \"a-size-base\"})\n",
    "            reviews = reviews_tag.text.strip() if reviews_tag else None\n",
    "\n",
    "            # Extract brand\n",
    "            brand = title.split()[0] if title != \"Title Not Available\" else None\n",
    "\n",
    "            # Append the extracted data\n",
    "            data.append({\n",
    "                \"Title\": title,\n",
    "                \"Price\": price,\n",
    "                \"Original Price\": original_price,\n",
    "                \"Discount (%)\": discount,\n",
    "                \"Rating\": rating,\n",
    "                \"Reviews\": reviews,\n",
    "                \"Brand\": brand,\n",
    "                \"Timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error while processing a product: {e}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape multiple pages\n",
    "all_data = []\n",
    "for page in range(1, 21):  # Adjust range to fetch enough products\n",
    "    print(f\"Fetching page {page}...\")\n",
    "    products = fetch_product_data(page)\n",
    "    all_data.extend(products)\n",
    "\n",
    "    # Stop if we have enough data\n",
    "    if len(all_data) >= 200:\n",
    "        break\n",
    "\n",
    "    # Pause to avoid overwhelming the server\n",
    "    time.sleep(2)\n",
    "\n",
    "# Save the data to a database file (append mode)\n",
    "database_file = \"amazon_products_database.csv\"\n",
    "df = pd.DataFrame(all_data[:200])  # Limit to 200 items\n",
    "\n",
    "# Append to the CSV file if it exists, otherwise create it\n",
    "try:\n",
    "    df.to_csv(database_file, mode='a', index=False, header=not pd.io.common.file_exists(database_file))\n",
    "    print(f\"Data appended to '{database_file}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving data to the database: {e}\")\n",
    "\n",
    "# Load the database and clean missing values\n",
    "def clean_database(file):\n",
    "    try:\n",
    "        # Load the database\n",
    "        data = pd.read_csv(file)\n",
    "        print(\"Database loaded successfully.\")\n",
    "\n",
    "        # Drop rows with missing values\n",
    "        data.dropna(inplace=True)\n",
    "\n",
    "        # Remove noise (e.g., rows with unrealistic prices or ratings)\n",
    "        data['Price'] = pd.to_numeric(data['Price'], errors='coerce')\n",
    "        data['Original Price'] = pd.to_numeric(data['Original Price'], errors='coerce')\n",
    "        data['Discount (%)'] = pd.to_numeric(data['Discount (%)'], errors='coerce')\n",
    "        data['Rating'] = pd.to_numeric(data['Rating'], errors='coerce')\n",
    "        data['Reviews'] = pd.to_numeric(data['Reviews'], errors='coerce')\n",
    "\n",
    "        data.dropna(inplace=True)  # Drop rows with non-numeric data converted to NaN\n",
    "\n",
    "        # Filter for valid numeric ranges\n",
    "        data = data[(data['Price'] > 0) & (data['Rating'] <= 5)]\n",
    "\n",
    "        # Save the cleaned data back to the database\n",
    "        data.to_csv(file, index=False)\n",
    "        print(\"Database cleaned and saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning the database: {e}\")\n",
    "\n",
    "# Clean the database\n",
    "clean_database(database_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edba2abe-4601-4edd-8298-aa20230aac73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
